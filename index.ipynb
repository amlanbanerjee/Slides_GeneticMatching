{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# **Statistical Matching**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ** <span style=\"color:darkred\"> What is Statistical Matching? </span> **\n",
    "\n",
    "- When treatment assignment is non-random, the treatmnent and control groups are drawn from different populations. Failure to correct for the resulting baseline groups differences can lead to biased estimates.\n",
    "\n",
    "\n",
    "- **Matching** is a method in quasi-experimental design framework to adjust for imbalance in baseline covariates of treatment and control groups due to the non-random assignment of the programs under evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  <span style=\"color:darkred\"> Commonly Used Methods </span> \n",
    "\n",
    "* **Propensity Score Matching**\n",
    "* **Mahalanobis Distance Matching**\n",
    "\n",
    "\n",
    "Both methods are built on specific notions of distance between observations of pretreatment covariates.    \n",
    "\n",
    "The most common implementation of each approach is to apply one-to-one nearest neighbor greedy matching without replacement.    \n",
    "\n",
    "Each approach matches each treated unit to the nearest control unit without replacement, using that method's chosen distance metric.   \n",
    "\n",
    "Some procedure, such as calipers, is then applied to remove treated units that are unreasonably distant from the control units to which they were matched, based on chosen cutoffs for the maximum distance allowed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <span style=\"color:darkred\"> Propensity Score Matching </span>\n",
    "\n",
    "- Propensity score matching (PSM) involves matching each treated unit to the nearest control unit on the **unidimensional** metric of the propensity score vector.\n",
    "\n",
    "\n",
    "- PSM is a way to condition on covariates $(\\mathbf{X})$ to match on the probability of assignment to treatment.\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{Propensity Score of unit } i: e(\\mathbf{X}_i) = Pr(T_i=1|\\mathbf{X}_i)\n",
    "$$\n",
    "\n",
    "\n",
    "- Matching on the true propensity score will asymptotically balance the observed covariates. Since the correct propensity score model is generally unknown, a misspecified propensity score model increases the imbalance of some observed variables post-matching, especially if the covariates have non-normal distributions.\n",
    "\n",
    "\n",
    "- Therefore, it is important to assess covariate balance in the matched sample and iteratively modify the propensity score model with the aim of balancing the covariates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <span style=\"color:darkred\"> Mahalanobis Distance Matching </span>\n",
    "\n",
    "- Mahalanobis Distance (MD) is a scaler quantity which measures the **multivariate** distance between individuals in different groups. \n",
    "\n",
    "- The MD between the $\\mathbf{X}$ covariates for two units $i$ and $j$ is \n",
    "\n",
    "$$\n",
    "MD(\\mathbf{X}_i, \\mathbf{X}_j) = \\sqrt{(\\mathbf{X}_i - \\mathbf{X}_j)'\\mathbf{S}^{-1}(\\mathbf{X}_i - \\mathbf{X}_j)}\n",
    "$$\n",
    "$$\n",
    "\\text { where } \\mathbf{S} \\text { is the sample convariance matrix of } \\mathbf{X}.\n",
    "$$\n",
    "- Like PSM, MD achieves covariate balance when covariates are normally distributed and sample size is large.\n",
    "\n",
    "- Rosenbaum and Rubin (1985)* recommend that \n",
    "\n",
    "    * in addition to PSM, one should match on individual covariates by minimizing the MD of $\\mathbf{X}$ to obtain balance on $\\mathbf{X}$; it can be done by including PS among $\\mathbf{X}$.\n",
    "    * alternatively, one may first match on the propensity score and then match based on MD within propensity score strata. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Rosenbaum, P. R. and D. B. Rubin (1985), \"Constructing a Control Group Using Multivariate Matched Sampling Methods That Incorporate the Propensity Score,\" The American Statistician, Vol. 39, No. 1 (Feb., 1985), pp. 33-38."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <span style=\"color:darkred\"> Challenges with PSM and MD Methods </span>\n",
    "\n",
    "1. If covariates are normally or $t-$distributed, PSM and MD have appealing theoretical properties, such as \"equal percent bias reduction (EPBR)\" property, which ensures that matching will reduce bias in all linear combination of the covariates. However, in reality, covariates are rarely normally distributed.\n",
    "    * If EPBR property is violated, matching will increase bias for some linear functions of the covariates even if all univariate means are closer to the matched data than the unmatched. \n",
    "    \n",
    "2. Misspecified propensity score model may increase the imbalance of some observed variables post-matching, especially if the covariates have non-normal distribution.\n",
    "\n",
    "3. Building a propensity score model is an iterative process, in which many candidate models are estimated and sequentially learned from one specification to the next. Hence the process of iteratively modifying the propensity score to maximize balance is often challenging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <span style=\"color:darkred\"> What is Genetic Matching? </span>\n",
    "\n",
    "Genetic Matching is a generalization of propensity score and Mahalanobis Distance matching methods. It is a multivariate matching method that uses an evolutionary search algorithm developed by Mebane and Sekhon (1985) to maximize the balance of observed covariates across matched treated and control units. \n",
    "\n",
    "Genetic Matching iteratively check and improve covariate balance automatically and eliminates the need to manually and iteratively check the propensity score.\n",
    "\n",
    "The algorithm optimizes the balance as much as possible, given the data. The method is nonparametric and does not depend on knowing or estimating the propensity score.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Reference (GM)\n",
    "\n",
    "\n",
    "Diamond, A., and J. S. Sekhon (2012). “Genetic Matching for Estimating Causal Effects: A General Multivariate Matching Method for Achieving Balance in Observational Studies.” Review of Economics and Statistics, 95(3): 932-945. \n",
    "  \n",
    "Mebane, W. R. Jr., and J. S. Sekhon (1998). “GENetic Optimization Using Derivatives (GENOUD).” Software Package. http://sekhon.berkeley.edu/rgenoud/\n",
    "\n",
    "Sekhon,  J. S. and W. R. Mebane, Jr. (1998).”Genetic Optimization Using Derivatives: Theory and Application to Nonlinear Models.” Political Analysis, 7: 189-203.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <span style=\"color:darkred\"> Genetic Matching </span>\n",
    "\n",
    "Given two covariates, $\\mathbf{X}_i$ and $\\mathbf{X}_j$, a generalized version of Mahalanobis Distance (GMD) can be expressed as:\n",
    "\n",
    "$$\n",
    "GMD(\\mathbf{X}_i, \\mathbf{X}_j) = \\sqrt{(\\mathbf{X}_i - \\mathbf{X}_j)'(\\mathbf{S}^{-1/2})'\\mathbf{W}(\\mathbf{S}^{-1/2})(\\mathbf{X}_i - \\mathbf{X}_j)}\n",
    "$$\n",
    "\n",
    "  where $\\mathbf{W}$ is a $k \\times k$ positive definite diagonal weight matrix and $\\mathbf{S}^{-1/2}$ is the Cholesky decomposition of $\\mathbf{S}.$    \n",
    "\n",
    "- The elements of $\\mathbf{W}$ are chosen by a **genetic search** algorithm to simultaneously minimize the distributional difference and location difference of covariates between the treatment and control groups.\n",
    "\n",
    "- The algorithm automates the iterative process of checking and improving overall covariate balance and guarantees asymptotic convergence to the optimal matched sample. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <span style=\"color:darkred\"> Genetic Search Algorithm </span>\n",
    "\n",
    "\n",
    "- At every iteration of optimization, the algorithm proposes a batch (**generation**) of weights, $\\mathbf{W}$, and moves towards to produce a subsequent generation with better candidate $\\mathbf{W}s,$ which minimize largest observed covariare discrepency by maximizing the smallest $p-value \\space$ observed across a series of standardized statistics. \n",
    "\n",
    "- Now, the size of each generation is the **population size** (say, 1000) and is constant for all generations. For each generation, the sample is matched according to a distance metric ($\\mathbf{W}$), producing as many matched samples as the population size.  \n",
    "\n",
    "- The loss function is evaluated for each matched sample, and the algorithm identifies the weights corresponding to the minimum loss and are sampled at an exponential rate for inclusion in the subsequent population. \n",
    "\n",
    "- Thus, the generation of candidate trials evolves towards containing, on average, better $\\mathbf{W}s,$ and asymtotically converges towards optimal solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "####  Balance Statistics - Genetic Matching Output in R\n",
    "\n",
    "![Genetic Matching Output in R](GM_pic.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Genetic Matching Output in R](GM_qq_before.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Genetic Matching Output in R](GM_qq_after.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <span style=\"color:darkred\"> MNA's Implementation of Genetic Matching </span>\n",
    "\n",
    "We implemented GM to match treatment and control groups on observational student demographic data at about ten community colleges in FL, AZ, and OH for the evaluation of Department of Labor TAACCCT grant.  \n",
    "\n",
    "On most covariates, we were able to reduce standard mean differences close to 0 post-matching. In general, we noticed that larger sample size yielded better covariance balance.  \n",
    "\n",
    "While we did not estimate propensity score models, literature shows that Genetic Matching dominates propensity score matching on measures of covariate balance. Sekhon and Grieve (2011) showed that when the true propensity score is unknown, Genetic Matching can lead to less bias and lower RMSE.\n",
    "\n",
    "Sekhon, J. S. and R. Grieve (2011). “A Nonparametric Matching Method for Covariate Adjustment with Application to Economic Evaluation, Health Economics. 21(6): 695-7142."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A Gentle Introduction to ![R logo](R.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## R is the best statistical software that money can't buy \n",
    "\n",
    "SPSS, SAS, and STATA users are limited in their ability to change their environment. They have to rely on algorithms that have been developed for them. The way they approach a problem is constrained by how SAS/SPSS/STATA employed programmers thought to approach them. And they have to pay money to use these constraining algorithms.\n",
    "\n",
    "R users are like wizards. They can rely on functions (spells) that have been developed for them by statistical researchers, but they can also create their own. They don’t have to pay for the use of them, and once experienced enough, they are almost unlimited in their ability to change their environment. \n",
    "\n",
    "Innovations happen in R almost instanteneously since researchers use R as their base language. These techniques are implemented in SPSS/SAS/STATA years later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Advantages\n",
    "\n",
    "* Fast and free\n",
    "* State of the art\n",
    "* Awesome graphics\n",
    "* One stop shopping - almost every analytical tool you can think of is available\n",
    "* Many advanced methods such as bayesian and ML computations are only available in R\n",
    "* Active user community\n",
    "* Excellent for simulation, programming, computer intensive analyses, etc.\n",
    "* Interfaces with SQL, Python, C, C++, FORTRAN\n",
    "\n",
    "## Disadvantages\n",
    "\n",
    "* Steep learnings curve - \"lot of frustration if you do not enjoy coding\"\n",
    "* No commercial support or no single manual - you are on your own\n",
    "* Easy to make mistakes if you don't know what you are doing\n",
    "* Working with large datasets limited by RAM - real problem!\n",
    "* Data wrangling can be messier and more mistake prone than SPSS, SAS, or STATA\n",
    "* There are about 1000 packages - as the number of packages grows, it is becoming difficult to choose the best package for your needs, and QC is an issue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recommended Book\n",
    "\n",
    "![R Book](R_book.jpg)\n",
    "\n",
    "This is a good start, but frankly forums like stackoverflow along with youtube videos, coursera and edx courses come to rescue. A book written today will most likely be outdated in two years since R is evolving every day - literally. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## R for Program Evaluation\n",
    "\n",
    "![R Eval Book](R_eval.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "“Using R is a bit akin to smoking. The beginning is difficult, one may get headaches and even gag the first few times. But in the long run, it becomes pleasurable and even addictive. Yet, deep down, for those willing to be honest, there is something not fully healthy in it.” --Francois Pinard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Resources:\n",
    "\n",
    "https://medium.com/learning-machine-learning/present-your-data-science-projects-with-jupyter-slides-75f20735eb0f\n",
    "jupyter nbconvert --to slides index.ipynb --reveal-prefix=reveal.js --SlidesExporter.reveal_theme=serif \n",
    "--SlidesExporter.reveal_scroll=True \n",
    "--SlidesExporter.reveal_transition=none\n",
    "\n",
    "reveal.js submodule creation\n",
    "https://gist.github.com/viegelinsch/1885ad4c3f2c85b615b946b8aa5d6738\n",
    "\n",
    "https://www.youtube.com/watch?v=wTphObvEH9w"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
